{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047a51f4",
   "metadata": {
    "id": "047a51f4"
   },
   "source": [
    "## CP 218 Worksheet 6\n",
    "\n",
    "### Bayesian Logistic Regression: MAP and Laplace Approximation\n",
    "\n",
    "In this workshop, we will learn about approximate Bayesian inference in logistic regression. Particularly, we will focus on MAP estimation and Laplace approximation. We will use a toy dataset and do the following:\n",
    "\n",
    "1. Implement MAP estimation and use it to compute the MAP solution\n",
    "2. Compute the Laplace Approximation for the toy logistic regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df43be56",
   "metadata": {
    "id": "df43be56"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from numpy.linalg import inv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315cb83",
   "metadata": {
    "id": "a315cb83"
   },
   "source": [
    "We first generate a 2D synthetic dataset with two different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a39935",
   "metadata": {
    "id": "65a39935"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2,\n",
    "                  random_state=42, cluster_std=4)\n",
    "\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec3691",
   "metadata": {
    "id": "50ec3691"
   },
   "outputs": [],
   "source": [
    "# plot the training data set.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.set_title('Synthetic logreg Data')\n",
    "\n",
    "class_one = Xtrain[ytrain == 0, :]\n",
    "class_two = Xtrain[ytrain == 1, :]\n",
    "\n",
    "ax.plot(class_one[:,0], class_one[:,1], 'bo', alpha=0.5, label='Class One')\n",
    "ax.plot(class_two[:,0], class_two[:,1], 'gs', alpha=0.5, label='Class Two')\n",
    "\n",
    "ax.set_xlabel(\"x_1\"); ax.set_ylabel(\"x_2\")\n",
    "ax.legend()\n",
    "ax.set_ylim([-3, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a940c0",
   "metadata": {
    "id": "05a940c0"
   },
   "source": [
    "### Model Specification\n",
    "\n",
    "The logistic regression model assumes that the outputs y are distributed as\n",
    "\n",
    "$$p(y|w) \\sim Bernoulli(\\sigma(\\boldsymbol{w}^\\top x))$$\n",
    "\n",
    "where $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ is the logistic function. The probability $p(y=1) = \\sigma(\\boldsymbol{w}^\\top x)$ is a linear function of the inputs $x$ passed through the logistic function $\\sigma(z)$ in this case --- whose range is the interval $[0,1]$.\n",
    "\n",
    "In our Bayesian logistic regression formulation, the likelihood of model is then given by,\n",
    "\n",
    "$$p(\\mathcal{D} | \\boldsymbol{w})  = \\prod_{n=1}^N Bernoulli(y_i | \\sigma(\\boldsymbol{w}^\\top x_i)) = \\prod_{n=1}^N \\sigma(\\boldsymbol{w}^\\top x_i)^{y_i}(1 - \\sigma(\\boldsymbol{w}^\\top x_i))^{(1 - y_i)}.$$\n",
    "\n",
    "We choose a Gaussian prior just as we did in linear regression,\n",
    "\n",
    "$$p(w) = \\mathcal{N}(\\boldsymbol{w} | p_m, \\lambda^{-1} \\boldsymbol{I}),$$\n",
    "\n",
    "where $\\lambda$ is the prior precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fdccb1",
   "metadata": {
    "id": "71fdccb1"
   },
   "outputs": [],
   "source": [
    "prior_mean = np.zeros(Xtrain.shape[1]) #p_m\n",
    "lam = 0.1\n",
    "prior_variance = np.eye(Xtrain.shape[1])/lam   #p_v\n",
    "prior_mean\n",
    "prior_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185efce",
   "metadata": {
    "id": "f185efce"
   },
   "source": [
    "#### The Laplace Approximation using MAP Estimation\n",
    "\n",
    "#### MAP Estimation\n",
    "\n",
    "We will now learn about the Laplace Approximation, which uses the MAP estimate to compute an approximation to the posterior distribution. Recall that MAP estimation computes the maximum of the posterior distribution (or joint likelihood distribution)\n",
    "\n",
    "$$\\mathcal{L}_{MAP} = \\log p(\\mathcal{D} | \\boldsymbol{w}) + \\log p(\\boldsymbol{w}) .$$\n",
    "\n",
    "This is achieved by minimizing the negative of the MAP objective:\n",
    "\n",
    "$$ \\boldsymbol{w}_{MAP} = argmin_{\\boldsymbol{w}} (- \\mathcal{L}_{MAP}) = argmin_{\\boldsymbol{w}} (- \\log p(\\mathcal{D} | \\boldsymbol{w}) - \\log p(\\boldsymbol{w}))$$\n",
    "\n",
    "In the following, we provide a training loop for minimizing the MAP objective and and implementation of the log-likelihood along with its gradients.\n",
    "\n",
    "\n",
    "Note that the multivariate Gaussian density is\n",
    "\n",
    "$$ p(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}, \\Sigma) = \\frac{1}{(|2\\pi \\Sigma|)^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{\\mu})^\\top \\Sigma^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu})\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a6d5b",
   "metadata": {
    "id": "373a6d5b"
   },
   "source": [
    "First we will Implement ```log joint likelihood function```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9edb75",
   "metadata": {
    "id": "7c9edb75"
   },
   "outputs": [],
   "source": [
    "def logJointLikelihood(X, t, w, p_m, p_v):\n",
    "\n",
    "    \"\"\"Computes the value of the negative log joint likelihood fucntion.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "\n",
    "        Training data, where n_samples is the number of samples and\n",
    "\n",
    "        n_features is the number of features.\n",
    "\n",
    "\n",
    "    t : ndarray, shape (n_samples,)\n",
    "\n",
    "        Array of labels.\n",
    "\n",
    "    w : ndarray, shape (n_features,)\n",
    "\n",
    "        Coefficient vector.\n",
    "\n",
    "\n",
    "    p_m: array-like, shape (n_features, )\n",
    "\n",
    "        The mean of prior.\n",
    "\n",
    "\n",
    "    p_v: {array-like, sparse matrix}, shape (n_features, n_features)\n",
    "\n",
    "        The covariance of prior.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    NeglogJointLikelihood : float, optional\n",
    "\n",
    "        The value of the negative log joint likelihood fucntion.\n",
    "\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "\n",
    "    Bishop, C. M. (2006). Pattern recognition and machine learning.\n",
    "\n",
    "    Springer. (Chapter 4.5.1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    p = 1/(1+np.exp(-X.dot(w)))\n",
    "    loglikelihood =  #fill in\n",
    "    logPrior = np.log(multivariate_normal.pdf(w, p_m, p_v))\n",
    "\n",
    "    logJointLikelihood = (loglikelihood + logPrior)\n",
    "    NeglogJointLikelihood= -1*logJointLikelihood\n",
    "\n",
    "    return NeglogJointLikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a0bb5",
   "metadata": {
    "id": "682a0bb5"
   },
   "source": [
    "Now we will Implement ```gradient of the log joint likelihood function```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90335d14",
   "metadata": {
    "id": "90335d14"
   },
   "outputs": [],
   "source": [
    "def logJointLikelihood_grad(X, t, w, p_m, p_v):\n",
    "\n",
    "    \"\"\"Computes the gradient of the log joint likelihood.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : {array-like,  matrix}, shape (n_samples, n_features)\n",
    "\n",
    "        Training data, where n_samples is the number of samples and\n",
    "\n",
    "        n_features is the number of features.\n",
    "\n",
    "\n",
    "    t : ndarray, shape (n_samples,)\n",
    "\n",
    "        Array of labels.\n",
    "\n",
    "    w : ndarray, shape (n_features,)\n",
    "\n",
    "        Weight Coefficient vector.\n",
    "\n",
    "\n",
    "    p_m: array-like, shape (n_features, )\n",
    "\n",
    "        The mean of prior.\n",
    "\n",
    "\n",
    "    p_v: {array-like, sparse matrix}, shape (n_features, n_features)\n",
    "\n",
    "        The covariance of prior.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    grad : ndarray, shape (n_features,)\n",
    "\n",
    "        Log joint likelihood gradient.\n",
    "\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    (cf. slide 14 of lecture 5 (CP 218) for prior term (consider mean p_m and lambda= inv(p_v) for prior)\n",
    "    \"\"\"\n",
    "\n",
    "    p = 1/(1 + np.exp(-X.dot(w)))\n",
    "\n",
    "    grad =   #fill in\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159390fe",
   "metadata": {
    "id": "159390fe"
   },
   "source": [
    "Remember that ```log_likelihood``` also computes the Hessian of $- \\log p(\\mathcal{D}|\\boldsymbol{w})$. Now we will Implement ```Hessian of the log joint likelihood```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48836053",
   "metadata": {
    "id": "48836053"
   },
   "outputs": [],
   "source": [
    "def logJointLikelihood_hess(X, t, w, p_m, p_v):\n",
    "\n",
    "    \"\"\"Computes the Hessian of the log joint likelihood.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : {array-like,  matrix}, shape (n_samples, n_features)\n",
    "\n",
    "        Training data, where n_samples is the number of samples and\n",
    "\n",
    "        n_features is the number of features.\n",
    "\n",
    "\n",
    "    t : ndarray, shape (n_samples,)\n",
    "\n",
    "        Array of labels.\n",
    "\n",
    "    w : ndarray, shape (n_features,)\n",
    "\n",
    "        Weight Coefficient vector.\n",
    "\n",
    "\n",
    "    p_m : array-like, shape (n_features, )\n",
    "\n",
    "        The mean of prior.\n",
    "\n",
    "\n",
    "    p_v : {array-like, sparse matrix}, shape (n_features, n_features)\n",
    "\n",
    "        The covariance of prior.\n",
    "\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    hess : {array-like, sparse matrix}, shape (n_features, n_features)\n",
    "\n",
    "        The Hessian matrix\n",
    "\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "\n",
    "    Bishop, C. M. (2006). Pattern recognition and machine learning.\n",
    "\n",
    "    Springer. (Chapter 4.5.1)\n",
    "\n",
    "     Hessian matrix whose elements comprise the second derivatives of negative log-joint-likelihood (or negative log posterior)\n",
    "     with respect to the components of weight coefficient\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    p = 1/(1 + np.exp(-X.dot(w)))\n",
    "    hess =  #fill in\n",
    "\n",
    "    return hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1409cf2",
   "metadata": {
    "id": "d1409cf2"
   },
   "source": [
    "Now we will Implement ```Gradient Descent ``` and ```Newton ```  method for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db071c4",
   "metadata": {
    "id": "8db071c4"
   },
   "outputs": [],
   "source": [
    "def GradientDescent(X, t, w, p_m, p_v, eta, tol, max_iter):\n",
    "\n",
    "    \"\"\"Implement gradient descent method to find optimal point for minimizing the negative\n",
    "\n",
    "    log joint likelihood.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : {array-like,  matrix}, shape (n_samples, n_features)\n",
    "\n",
    "        Training data, where n_samples is the number of samples and\n",
    "\n",
    "        n_features is the number of features.\n",
    "\n",
    "\n",
    "    t : ndarray, shape (n_samples,)\n",
    "\n",
    "        Array of labels.\n",
    "\n",
    "\n",
    "    w : ndarray, shape (n_features,)\n",
    "\n",
    "        Weight Coefficient vector.\n",
    "\n",
    "\n",
    "    p_m: array-like, shape (n_features, )\n",
    "\n",
    "        The mean of prior.\n",
    "\n",
    "\n",
    "    p_v: {array-like, sparse matrix}, shape (n_features, n_features)\n",
    "\n",
    "        The covariance of prior.\n",
    "\n",
    "\n",
    "    eta: float, optional\n",
    "\n",
    "        The learning rate\n",
    "\n",
    "\n",
    "    tol : float, optional\n",
    "\n",
    "        Tolerance for stopping criteria.\n",
    "\n",
    "\n",
    "    max_iter: int, optional\n",
    "\n",
    "        Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "\n",
    "\n",
    "    Returns\n",
    "\n",
    "    -------\n",
    "\n",
    "    w_opt: ndarray, shape (n_features,)\n",
    "\n",
    "        The optimal coefficient vector.\n",
    "\n",
    "    losses: ndarray, optimal\n",
    "\n",
    "        Values of the objective function, saving for each iteration.\n",
    "\n",
    "    \"\"\"\n",
    "    log_joint_likelihood = 1e5\n",
    "    log_joint_likelihoods = []\n",
    "    w_history=[]\n",
    "    w_history.append(w)\n",
    "    w_opt = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "\n",
    "\n",
    "        grad = logJointLikelihood_grad(X, t, w, p_m, p_v)\n",
    "\n",
    "        # update the coefficient for next step\n",
    "        w_new =  w - eta*grad   #fill in\n",
    "        w_history.append(w_new)\n",
    "\n",
    "        # check convergence\n",
    "        log_joint_likelihood_new = logJointLikelihood(X, t, w_new, p_m, p_v)\n",
    "        if (abs(log_joint_likelihood_new - log_joint_likelihood) < tol):\n",
    "            break\n",
    "        else:\n",
    "            log_joint_likelihoods.append(log_joint_likelihood_new)\n",
    "            log_joint_likelihood, w = log_joint_likelihood_new, w_new\n",
    "\n",
    "    w_opt = w_new\n",
    "    return w_opt, np.array(log_joint_likelihoods), w_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927e8d3",
   "metadata": {
    "id": "8927e8d3"
   },
   "outputs": [],
   "source": [
    "def NewtonRaphson(X, t, w, p_m, p_v, eta, tol, max_iter):\n",
    "\n",
    "\n",
    "    \"\"\"Implement Newton Raphson method to find optimal point\n",
    "\n",
    "    for the minimizing the negative of log joint likelihood.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "\n",
    "        Training data, where n_samples is the number of samples and\n",
    "\n",
    "        n_features is the number of features.\n",
    "\n",
    "\n",
    "    t : ndarray, shape (n_samples,)\n",
    "\n",
    "        Array of labels.\n",
    "\n",
    "\n",
    "    w : ndarray, shape (n_features,)\n",
    "\n",
    "        Coefficient vector.\n",
    "\n",
    "\n",
    "    p_m: array-like, shape (n_features, )\n",
    "\n",
    "        The mean of prior.\n",
    "\n",
    "\n",
    "    p_v: {array-like, sparse matrix}, shape (n_features, n_features)\n",
    "\n",
    "        The covariance of prior.\n",
    "\n",
    "\n",
    "    eta: float, optional\n",
    "\n",
    "        The learning rate\n",
    "\n",
    "\n",
    "    tol : float, optional\n",
    "\n",
    "        Tolerance for stopping criteria.\n",
    "\n",
    "\n",
    "    max_iter: int, optional\n",
    "\n",
    "        Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "\n",
    "\n",
    "    Returns\n",
    "\n",
    "    -------\n",
    "\n",
    "    w_opt: ndarray, shape (n_features,)\n",
    "\n",
    "        The optimal coefficient vector.\n",
    "\n",
    "    losses: ndarray, optional\n",
    "\n",
    "        Values of the objective function, saving for each iteration.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # hyper-parameters setting (Note: 'loss' is set to large value to avoid first time stop)\n",
    "    log_joint_likelihood = 1e5\n",
    "    log_joint_likelihoods = []\n",
    "    w_opt = 0\n",
    "    w_history= []\n",
    "    w_history.append(w)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "\n",
    "        # find the next step length.\n",
    "        grad = logJointLikelihood_grad(X, t, w, p_m, p_v)\n",
    "        hess = logJointLikelihood_hess(X, t, w, p_m, p_v)\n",
    "\n",
    "        # update the coefficient for next step\n",
    "        w_new =    #fill in\n",
    "        w_history.append(w_new)\n",
    "\n",
    "        # check convergence\n",
    "        log_joint_likelihood_new = logJointLikelihood(X, t, w_new, p_m, p_v)\n",
    "        if (abs(log_joint_likelihood_new - log_joint_likelihood) < tol):\n",
    "            break\n",
    "        else:\n",
    "            log_joint_likelihoods.append(log_joint_likelihood_new)\n",
    "            log_joint_likelihood, w = log_joint_likelihood_new, w_new\n",
    "\n",
    "        w_opt = w_new\n",
    "\n",
    "    return w_opt, log_joint_likelihoods, w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4dc15a",
   "metadata": {
    "id": "7b4dc15a"
   },
   "source": [
    "Now we will create a ```BayesianLogistic``` class which implements Bayesian version of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d986264",
   "metadata": {
    "id": "2d986264"
   },
   "outputs": [],
   "source": [
    "class BayesianLogistic(object):\n",
    "\n",
    "\n",
    "    \"\"\"Bayesian Logistic Regression classifier.\n",
    "\n",
    "    This class implements Bayesian version of logistic regression.\n",
    "\n",
    "    Input with defined prior distribution over parameter, it updates\n",
    "\n",
    "    the posterior distribution by the given dataset. Moreover, the posterior\n",
    "\n",
    "    is updated by using two methods: point estimate (MAP) and Laplace approximation.\n",
    "\n",
    "    The former is optimized by gradient descent or Newton-Raphson techiniques.\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, p_m, p_v, solver='newton', eta=1e-3, tol=1e-5, max_iter=100):\n",
    "\n",
    "        \"\"\"Intialization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_m: array-like, shape (n_features, )\n",
    "\n",
    "            The mean of prior, where n_features is the number of features.\n",
    "\n",
    "\n",
    "        p_v: {array-like, sparse matrix}, shape (n_features, n_features)\n",
    "\n",
    "            The covariance of prior, where n_features is the number of features.\n",
    "\n",
    "\n",
    "        solver : str, {'newton', 'gd'}, optional (default='newton').\n",
    "\n",
    "        Algorithm to use in the optimization problem.\n",
    "\n",
    "        - 'newton' implement Newton-Raphson method, fast converge but need more computation (Hessian matrix)\n",
    "\n",
    "        - 'gd' implement gradient descent method\n",
    "\n",
    "\n",
    "        eta: float, optional (default=1e-3)\n",
    "\n",
    "            The learning rate\n",
    "\n",
    "\n",
    "        tol : float, optional (default=1e-5)\n",
    "\n",
    "            Tolerance for stopping criteria.\n",
    "\n",
    "\n",
    "        max_iter: int, optional (default=100)\n",
    "\n",
    "            Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "\n",
    "        coef_ : array, shape (1, n_features)\n",
    "\n",
    "            Coefficient of the features in the decision function when using MAP estimate.\n",
    "\n",
    "        log_joint_likelihood_ : ndarray, optional\n",
    "\n",
    "            Values of the objective function (log_joint_likelihood), saving for each iteration.\n",
    "\n",
    "        \"\"\"\n",
    "        self.p_m = p_m\n",
    "        self.p_v = p_v\n",
    "        self.solver = solver\n",
    "        self.eta = eta\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "\n",
    "    def fit(self, X, t):\n",
    "\n",
    "\n",
    "        \"\"\"Fit the model according to the given training data.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "\n",
    "            n_features is the number of features.\n",
    "\n",
    "\n",
    "        t : array-like, shape (n_samples,)\n",
    "\n",
    "            Target vector relative to X.\n",
    "\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        self : object\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        w_init = 1e-3*np.ones(X.shape[1])\n",
    "\n",
    "        if (self.solver == 'gd'):\n",
    "            self.coef_, self.log_joint_likelihood_, self.weight_history_ = GradientDescent(X, t, w_init, self.p_m, self.p_v, self.eta, self.tol, self.max_iter)\n",
    "        else:\n",
    "            self.coef_, self.log_joint_likelihood_, self.weight_history_ = NewtonRaphson(X, t, w_init, self.p_m, self.p_v, self.eta, self.tol, self.max_iter)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        \"\"\"predict using the logistic model\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        T : array-like, shape = [n_samples,]\n",
    "\n",
    "            Returns the class of the sample.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        pred = []\n",
    "\n",
    "        for p in 1/(1+np.exp(-X.dot(self.coef_))):\n",
    "            if p <= 0.5:\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "\n",
    "        return np.array(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f324958",
   "metadata": {
    "id": "8f324958"
   },
   "source": [
    "Now we have all the functions we need. So, let's apply Bayesian logistic regression on training data (Newton-Raphson method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981f42f",
   "metadata": {
    "id": "f981f42f"
   },
   "outputs": [],
   "source": [
    "model = BayesianLogistic(p_m=prior_mean, p_v=prior_variance, solver='newton', eta=1, max_iter=10) #try changing eta and max_iter for better convergence\n",
    "model.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac915cca",
   "metadata": {
    "id": "ac915cca"
   },
   "outputs": [],
   "source": [
    "np.array(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa30027",
   "metadata": {
    "id": "0aa30027"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0caad680",
   "metadata": {
    "id": "0caad680"
   },
   "source": [
    "Let's see how negative log_joint_likelihood changes in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de2155",
   "metadata": {
    "id": "75de2155"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(len(model.log_joint_likelihood_)), np.array(model.log_joint_likelihood_))\n",
    "plt.xlabel(\"Iterations\", weight=\"semibold\", color='b', size=15)\n",
    "plt.ylabel(\"Negative Log joint likelihood\", weight=\"semibold\", color='b', size=15)\n",
    "plt.xticks(range(len(model.log_joint_likelihood_)))\n",
    "plt.title(\"Newton Raphson Optimization\", weight=\"semibold\", color='r', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faffeb16",
   "metadata": {
    "id": "faffeb16"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(Xval)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8c34a",
   "metadata": {
    "id": "c0d8c34a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf854a09",
   "metadata": {
    "id": "cf854a09"
   },
   "outputs": [],
   "source": [
    "###model.weight_history_\n",
    "###weight_tensor = torch.tensor(model.weight_history_, requires_grad=True, dtype=torch.float)\n",
    "###weight_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45be5af",
   "metadata": {
    "id": "f45be5af"
   },
   "outputs": [],
   "source": [
    "def plot_map_boundary(ax1, w):\n",
    "    if w[1] == 0:\n",
    "        boundary = 0\n",
    "    else:\n",
    "        boundary = - w[0] / w[1]\n",
    "\n",
    "    x1 = np.arange(-7,4)\n",
    "\n",
    "    ax1.plot(x1, boundary * x1, 'r', lw=2, alpha=0.5, label='Decision Boundary')\n",
    "    ax1.legend()\n",
    "\n",
    "    return ax1\n",
    "\n",
    "\n",
    "\n",
    "# plot the training data set.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "class_one = Xtrain[ytrain == 0, :]\n",
    "class_two = Xtrain[ytrain == 1, :]\n",
    "\n",
    "ax.plot(class_one[:,0], class_one[:,1], 'bo', alpha=0.5, label='Class One')\n",
    "ax.plot(class_two[:,0], class_two[:,1], 'gs', alpha=0.5, label='Class Two')\n",
    "\n",
    "ax.set_xlabel(\"x_1\"); ax.set_ylabel(\"x_2\")\n",
    "plot_map_boundary(ax, np.array(model.coef_))\n",
    "ax.set_ylim([-3, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee3e5c",
   "metadata": {
    "id": "20ee3e5c"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, cmap=plt.cm.Blues):\n",
    "\n",
    "    \"\"\"Plot the confusion matrix.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ytrue : array-like, shape = [n_samples, ]\n",
    "\n",
    "        The true value , where n_samples is the number of samples.\n",
    "\n",
    "    ypred : array-like, shape = [n_samples, ]\n",
    "\n",
    "        The predicted value\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalize = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "    ax[0].imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax[1].imshow(cm_normalize, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "    # Show all ticks\n",
    "    ax[0].set_xlabel(\"Predicted label\", weight=\"semibold\", color='b', size=15)\n",
    "    ax[0].set_ylabel(\"True label\", weight=\"semibold\", color='b', size=15)\n",
    "    ax[0].set_title(\"Confusion matrix, without normalization\", weight=\"semibold\", color='r', size=15)\n",
    "    ax[0].set_xticks(classes)\n",
    "    ax[0].set_yticks(classes)\n",
    "\n",
    "    ax[1].set_xlabel(\"Predicted label\", weight=\"semibold\", color='b', size=15)\n",
    "    ax[1].set_ylabel(\"True label\", weight=\"semibold\", color='b', size=15)\n",
    "    ax[1].set_title(\"Confusion matrix, with normalization\", weight=\"semibold\", color='r', size=15)\n",
    "    ax[1].set_xticks(classes)\n",
    "    ax[1].set_yticks(classes)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax[0].text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\", color=\"r\", fontsize=30, weight=\"semibold\")\n",
    "            ax[1].text(j, i, format(cm_normalize[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"r\", fontsize=30, weight=\"semibold\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(yval, pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da45b7",
   "metadata": {
    "id": "83da45b7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(pred, yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064d680",
   "metadata": {
    "id": "0064d680"
   },
   "source": [
    "Q: What do you think about the accuracy for this dataset? Can we improve it further? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63559a43",
   "metadata": {
    "id": "63559a43"
   },
   "source": [
    "We can also adaptive choose learning rate. Please read Chapter 9.3 of the Stephen Boyd and Lieven Vandenberghe (2004). Convex Optimization.\n",
    "Cambridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36700a84",
   "metadata": {
    "id": "36700a84"
   },
   "source": [
    "### The Laplace Approximation\n",
    "\n",
    "We now consider computing the Laplace Approximation to the posterior distribution for logistic regression problem. This is easy to do since we have already found MAP estimate of our model parameters, $\\boldsymbol{w}_{MAP}$, using gradient descent.\n",
    "\n",
    "Recall that the Laplace approximation is found by a second order Taylor series approximation of the log-posterior distribution at the MAP estimate:\n",
    "\n",
    "$$ \\log p(\\boldsymbol{w} | \\mathcal{D}) \\approx \\log p(\\boldsymbol{w}_{MAP} | \\mathcal{D}) + (\\boldsymbol{w} - \\boldsymbol{w}_{MAP})^\\top \\nabla_{\\boldsymbol{w}} \\log p(\\boldsymbol{w}_{MAP} | \\mathcal{D}) +  \\frac{1}{2} (\\boldsymbol{w} - \\boldsymbol{w}_{MAP})^\\top \\nabla_{\\boldsymbol{w}^2}^2 \\log p(\\boldsymbol{w}_{MAP} | \\mathcal{D})(\\boldsymbol{w} - \\boldsymbol{w}_{MAP}) $$\n",
    "\n",
    " Remember that the covariance matrix for (approximated) posterior distribution is the inverse of Hessian of $- \\log p(\\mathcal{D}|\\boldsymbol{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f95c75",
   "metadata": {
    "id": "41f95c75"
   },
   "outputs": [],
   "source": [
    "class BayesianLogistic_LaplaceApproximation(BayesianLogistic):\n",
    "\n",
    "\n",
    "    \"\"\"Bayesian Logistic Regression classifier.\n",
    "\n",
    "    This class inherit BayesianLogistic class. Futhermore, it is extend by\n",
    "\n",
    "    apply Laplace approximation to find a Gaussian approximation to a posterior distribution\"\"\"\n",
    "\n",
    "    def LaplaceApproximation(self, X, t):\n",
    "\n",
    "        \"\"\"predict using the logistic model\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "\n",
    "            n_features is the number of features.\n",
    "\n",
    "\n",
    "        t : array-like, shape (n_samples,)\n",
    "\n",
    "            Target vector relative to X.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        w_init = 1e-3*np.ones(X.shape[1])\n",
    "        self.pos_m, _, _ = NewtonRaphson(X, t, w_init, self.p_m, self.p_v, self.eta, self.tol, self.max_iter)#(use Newton's method)\n",
    "        self.pos_v = inv(logJointLikelihood_hess(X, t, self.pos_m, self.p_m, self.p_v))  #fill in\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2f4ff",
   "metadata": {
    "id": "65c2f4ff"
   },
   "outputs": [],
   "source": [
    "# prior distribution setting\n",
    "prior_variance = np.eye(Xtrain.shape[1])/lam\n",
    "prior_mean = np.zeros(Xtrain.shape[1])\n",
    "\n",
    "# apply Bayesian logistic regression on training data\n",
    "model = BayesianLogistic_LaplaceApproximation(p_m=prior_mean, p_v=prior_variance, eta=0.1, max_iter=100)\n",
    "model = model.LaplaceApproximation(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62a411",
   "metadata": {
    "id": "3d62a411"
   },
   "outputs": [],
   "source": [
    "# mean of Laplace approximation distribution\n",
    "model.pos_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2c423",
   "metadata": {
    "id": "04a2c423"
   },
   "outputs": [],
   "source": [
    "# variance of Laplace approximation distribution\n",
    "model.pos_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50b5aa",
   "metadata": {
    "id": "2a50b5aa"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "# set up a 2d plot mesh\n",
    "w1, w2 = np.mgrid[-1:1:.05, -1:1:.05]\n",
    "grid = np.c_[w1.ravel(), w2.ravel()]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "# plot a bivariate normal for the prior\n",
    "ax = fig.add_subplot(121)\n",
    "p_w = sp.stats.multivariate_normal.pdf(grid, mean=np.zeros(2), cov=prior_variance)\n",
    "CS = ax.contour(w1, w2, p_w.reshape(w1.shape))\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "ax.plot(0, 0, 'rx') # add prior mean\n",
    "plt.xlabel('$w_1$')\n",
    "plt.ylabel('$w_2$')\n",
    "plt.title('Prior $p(w_1, w_2|\\lambda)$')\n",
    "\n",
    "# plot a bivariate normal for the posterior\n",
    "ax = fig.add_subplot(122)\n",
    "p_w = sp.stats.multivariate_normal.pdf(grid, mean= model.pos_m, cov= model.pos_v)\n",
    "CS = ax.contour(w1, w2, p_w.reshape(w1.shape))\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "ax.plot(model.pos_m[0], model.pos_m[1], 'rx') # add posterior mean\n",
    "plt.xlabel('$w_1$')\n",
    "plt.ylabel('$w_2$')\n",
    "plt.title('Posterior $p(w_1, w_2|X,y)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad3a5f",
   "metadata": {
    "id": "88ad3a5f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
